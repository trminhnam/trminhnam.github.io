---
---
@inproceedings{tran-etal-2024-dual,
  abbr      = {Dual-Level Learning},
  title     = {Dual-Level Learning for Vietnamese Medical Natural Language Inference},
  author    = {Tran, Minh-Nam  and
               Nguyen, Phu-Vinh  and
               Nguyen, Long  and
               Dinh, Dien},
  booktitle = {review to conference},
  year      = {2024},
  preview   = {coling2025_dual-level.png},
  selected  = {true}
}

@inproceedings{tran-etal-2024-vimedaqa,
  abbr        = {ViMedAQA},
  title       = {{V}i{M}ed{AQA}: A {V}ietnamese Medical Abstractive Question-Answering Dataset and Findings of Large Language Model},
  author      = {Tran, Minh-Nam  and
                 Nguyen, Phu-Vinh  and
                 Nguyen, Long  and
                 Dinh, Dien},
  editor      = {Fu, Xiyan  and
                 Fleisig, Eve},
  booktitle   = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)},
  month       = aug,
  year        = {2024},
  address     = {Bangkok, Thailand},
  publisher   = {Association for Computational Linguistics},
  url         = {https://aclanthology.org/2024.acl-srw.31},
  doi         = {10.18653/v1/2024.acl-srw.31},
  pages       = {356--364},
  bibtex_show = {true},
  pdf         = {https://aclanthology.org/2024.acl-srw.31.pdf},
  html        = {https://aclanthology.org/2024.acl-srw.31},
  code        = {https://github.com/trminhnam/vimedaqa},
  preview     = {acl2024_vimedaqa.png},
  abstract    = {Question answering involves creating answers to questions. With the growth of large language models, the ability of question-answering systems has dramatically improved. However, there is a lack of Vietnamese abstractive question-answering datasets, especially in the medical domain. Therefore, this research aims to mitigate this gap by introducing ViMedAQA. This **Vi**etnamese **Med**ical **A**bstractive **Q**uestion-**A**nswering dataset covers four topics in the Vietnamese medical domain, including body parts, disease, drugs and medicine. Additionally, the empirical results on the proposed dataset examine the capability of the large language models in the Vietnamese medical domain, including reasoning, memorizing and awareness of essential information.}
}

@inproceedings{tran-etal-2024-viglue,
  abbr        = {ViGLUE},
  title       = {{V}i{GLUE}: A {V}ietnamese General Language Understanding Benchmark and Analysis of {V}ietnamese Language Models},
  author      = {Tran*, Minh-Nam  and
                 Nguyen*, Phu-Vinh  and
                 Nguyen, Long  and
                 Dinh, Dien},
  editor      = {Duh, Kevin  and
                 Gomez, Helena  and
                 Bethard, Steven},
  booktitle   = {Findings of the Association for Computational Linguistics: NAACL 2024},
  month       = jun,
  year        = {2024},
  address     = {Mexico City, Mexico},
  publisher   = {Association for Computational Linguistics},
  url         = {https://aclanthology.org/2024.findings-naacl.261},
  doi         = {10.18653/v1/2024.findings-naacl.261},
  pages       = {4174--4189},
  html        = {https://aclanthology.org/2024.findings-naacl.261},
  pdf         = {https://aclanthology.org/2024.findings-naacl.261.pdf},
  code        = {https://github.com/trminhnam/ViGLUE},
  selected    = {true},
  bibtex_show = {true},
  annotation  = {The dataset is available at [this link](https://huggingface.co/datasets/tmnam20/ViGLUE)},
  preview     = {naacl2024_viglue.png},
  abstract    = {As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to the difficulty in accessing natural language processing datasets or the scarcity of task-specific datasets. *ViGLUE*, the proposed dataset collection, is a Vietnamese General Language Understanding Evaluation benchmark developed using three methods: translating an existing benchmark, generating new corpora, and collecting available datasets. ViGLUE contains twelve tasks and encompasses over ten areas and subjects, enabling it to evaluate models comprehensively over a broad spectrum of aspects. Baseline models utilizing multilingual language models are also provided for all tasks in the proposed benchmarks. In addition, the study of the available Vietnamese large language models is conducted to explore the language models{'} ability in the few-shot learning framework, leading to the exploration of the relationship between specific tasks and the number of shots.}
}

@inproceedings{To_2024_CVPR,
  abbr        = {MultiView},
  author      = {To*, Tuan-An and Tran*, Minh-Nam and Ho*, Trong-Bao and Ha*, Thien-Loc and Nguyen*, Quang-Tan and Luong, Hoang-Chau and Cao, Thanh-Duy and Tran, Minh-Triet},
  title       = {Multi-perspective Traffic Video Description Model with Fine-grained Refinement Approach},
  booktitle   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month       = {June},
  year        = {2024},
  pages       = {7075-7084},
  html        = {https://openaccess.thecvf.com/content/CVPR2024W/AICity/html/To_Multi-perspective_Traffic_Video_Description_Model_with_Fine-grained_Refinement_Approach_CVPRW_2024_paper.html},
  pdf         = {https://openaccess.thecvf.com/content/CVPR2024W/AICity/papers/To_Multi-perspective_Traffic_Video_Description_Model_with_Fine-grained_Refinement_Approach_CVPRW_2024_paper.pdf},
  code        = {https://github.com/ToTuanAn/AICityChallenge2024_Track2},
  selected    = {true},
  bibtex_show = {true},
  preview     = {cvpr2024_MultiViewModel.png},
  abstract    = {The analysis of traffic patterns is crucial for enhancing safety and optimizing flow within urban cities. While urban cities possess extensive camera networks for monitoring the raw video data often lacks the contextual detail necessary for understanding complex traffic incidents and the behaviors of road users. This paper proposes a novel methodology for generating comprehensive descriptions of traffic scenarios combining a vision-language model (VLM) with rule-based refinements to capture pertinently pedestrian vehicle and environment factors. First a captioning model will generate a general description using processed video as input. Subsequently this description is refined sequentially through three primary modules: pedestrian-aware vehicle-aware and context-aware enhancing the final description. We evaluate our method on the Woven Traffic Safety datasets in Track 2 of the AI City Challenge 2024 obtaining competitive results with an S2 score of 22.6721. Code will be available at https://github.com/ToTuanAn/AICityChallenge2024_Track2}
}
